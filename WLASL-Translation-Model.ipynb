{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":7195414,"datasetId":4069519,"databundleVersionId":7284756},{"sourceType":"datasetVersion","sourceId":2632847,"datasetId":1589971,"databundleVersionId":2676713},{"sourceType":"datasetVersion","sourceId":4747505,"datasetId":2747345,"databundleVersionId":4810598}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abd0kamel/mutemotion-wlasl-translation-model?scriptVersionId=154915130\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Introduction**\n\n<h3> This notebook aims to use MediaPipe landmarks detection as the starting point\n    \n<h3> for building an American Sign Language Translation Model\n            \n**<h3> Sections:**\n* [Importing Libraries](#Importing_Libraries)\n* [Data Preparation](#Data_Preparation)\n* [MediaPipe Implementation](#MediaPipe_Implementation)\n* [Visualizing Landmarks](#Visualizing_Landmarks)\n* [Data Encoding](#Data_Encoding)\n* [Label Filtering](#Label_Filtering)\n* [Data Loading](#Data_Loading)\n* [Data Augmentation](#Data_Augmentation)\n* [Data Preprocessing](#Data_Preprocessing)\n* [Label Encoding](#Label_Encoding)\n* [Models](#Models)\n    \n<h4> If you found this notebook helpful, an upvote would be appreciated!","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Importing_Libraries\">\n    \n# **Importing Libraries** ","metadata":{}},{"cell_type":"code","source":"!pip install -q mediapipe==0.10.7","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-13T21:53:54.750535Z","iopub.execute_input":"2023-12-13T21:53:54.750995Z","iopub.status.idle":"2023-12-13T21:54:09.44613Z","shell.execute_reply.started":"2023-12-13T21:53:54.75096Z","shell.execute_reply":"2023-12-13T21:54:09.444418Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport json\nimport time\nimport shutil\nimport numpy as np\nfrom tqdm import tqdm\nimport mediapipe as mp\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom concurrent.futures import ThreadPoolExecutor\nfrom IPython.display import clear_output, FileLink","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-13T21:54:09.448453Z","iopub.execute_input":"2023-12-13T21:54:09.448827Z","iopub.status.idle":"2023-12-13T21:54:26.005479Z","shell.execute_reply.started":"2023-12-13T21:54:09.448796Z","shell.execute_reply":"2023-12-13T21:54:26.003661Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Data_Preparation\">\n    \n# **Data Preparation**\n**<h2>(Done Once)**","metadata":{}},{"cell_type":"markdown","source":"<h3> We first load the data","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/wlasl-processed/WLASL_v0.3.json', 'r') as json_file:\n    all_data = json.load(json_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:40:04.462788Z","iopub.execute_input":"2023-12-01T21:40:04.463388Z","iopub.status.idle":"2023-12-01T21:40:04.715599Z","shell.execute_reply.started":"2023-12-01T21:40:04.463358Z","shell.execute_reply":"2023-12-01T21:40:04.714639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Then we extract the needed information from the data\n<h4>    \n    \n* gloss : the word being expressed\n    \n* video_path : the path to the video in the datasets\n    \n* frame_start : the frame number where the word starts\n    \n* frame_end : the frame number where each word ends\n   \n* split : the type of data when modeling (train, val, test)","metadata":{}},{"cell_type":"code","source":"video_dir = '/kaggle/input/wlasl-processed/videos'\nbackup_dir = '/kaggle/input/wlasl2000-resized/wlasl-complete/videos'\ndata = [] # formatted data\n\nfor i in tqdm(range(len(all_data)), ncols=100):\n    gloss = all_data[i]['gloss']\n    instances = all_data[i]['instances']\n    for instance in instances:\n        video_id = instance['video_id']\n        if os.path.exists(os.path.join(video_dir, f'{video_id}.mp4')):\n            video_path = os.path.join(video_dir, f'{video_id}.mp4')\n        elif os.path.exists(os.path.join(backup_dir, f'{video_id}.mp4')):\n            video_path = os.path.join(backup_dir, f'{video_id}.mp4')\n        else:\n            continue\n            \n        frame_start = instance['frame_start']\n        frame_end = instance['frame_end']\n        split = instance['split']\n        data.append({\n            'gloss': gloss,\n            'video_path': video_path,\n            'frame_start': frame_start,\n            'frame_end': frame_end,\n            'split': split\n        })","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:40:04.71712Z","iopub.execute_input":"2023-12-01T21:40:04.717996Z","iopub.status.idle":"2023-12-01T21:41:13.368202Z","shell.execute_reply.started":"2023-12-01T21:40:04.717956Z","shell.execute_reply":"2023-12-01T21:41:13.367082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:41:13.371024Z","iopub.execute_input":"2023-12-01T21:41:13.371452Z","iopub.status.idle":"2023-12-01T21:41:13.37917Z","shell.execute_reply.started":"2023-12-01T21:41:13.371414Z","shell.execute_reply":"2023-12-01T21:41:13.378032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> We then save the organized dictionary for future uses","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/WLASL_parsed_data.json', 'w') as json_file:\n    json.dump(data, json_file, indent=4)\n    \nFileLink(r'WLASL_parsed_data.json')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:41:13.380775Z","iopub.execute_input":"2023-12-01T21:41:13.381358Z","iopub.status.idle":"2023-12-01T21:41:13.607781Z","shell.execute_reply.started":"2023-12-01T21:41:13.381322Z","shell.execute_reply":"2023-12-01T21:41:13.60672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"MediaPipe_Implementation\">\n\n# **MediaPipe Implementation**\n**<h2>(Not important when training)**","metadata":{}},{"cell_type":"markdown","source":"<h3> We first choose the landmarks\n<h4>    \n    \n* Hands : we'll keep all **42** of them, as they are the most important part.\n    \n* Pose : **6** landmarks for the upper body excluding the face, as we have dedicated process for it.\n    \n* Face : out of the **478** landmarks, we'll choose **132**, focusing on the lips, eyes, eyebrows, and the outline of the face.\n    \n* This brings the total number of landmarks to **180**, each with coordinates (x, y, z).   ","metadata":{}},{"cell_type":"code","source":"filtered_hand = list(range(21))\n\nfiltered_pose = [11, 12, 13, 14, 15, 16]\n\nfiltered_face = [0, 4, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58,\n                 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105,\n                 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154,\n                 155, 157, 158, 159, 160, 161, 162, 163, 172, 173, 176, 178, 181, 185, 191,\n                 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291,\n                 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324,\n                 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380,\n                 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409,\n                 415, 454, 466, 468, 473]\n\nHAND_NUM = len(filtered_hand)\nPOSE_NUM = len(filtered_pose)\nFACE_NUM = len(filtered_face)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:03:08.020837Z","iopub.execute_input":"2023-12-13T20:03:08.021775Z","iopub.status.idle":"2023-12-13T20:03:08.033336Z","shell.execute_reply.started":"2023-12-13T20:03:08.021735Z","shell.execute_reply":"2023-12-13T20:03:08.031977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Extracting landmarks from a frame","metadata":{}},{"cell_type":"code","source":"hands = mp.solutions.hands.Hands()\npose = mp.solutions.pose.Pose()\nface_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n\ndef get_frame_landmarks(frame):\n    \n    all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM + FACE_NUM, 3))\n    \n    def get_hands(frame):\n        results_hands = hands.process(frame)\n        if results_hands.multi_hand_landmarks:\n            for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n                if results_hands.multi_handedness[i].classification[0].index == 0: \n                    all_landmarks[:HAND_NUM, :] = np.array(\n                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # right\n                else:\n                    all_landmarks[HAND_NUM:HAND_NUM * 2, :] = np.array(\n                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # left\n\n    def get_pose(frame):\n        results_pose = pose.process(frame)\n        if results_pose.pose_landmarks:\n            all_landmarks[HAND_NUM * 2:HAND_NUM * 2 + POSE_NUM, :] = np.array(\n                [(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])[filtered_pose]\n        \n    def get_face(frame):\n        results_face = face_mesh.process(frame)\n        if results_face.multi_face_landmarks:\n            all_landmarks[HAND_NUM * 2 + POSE_NUM:, :] = np.array(\n                [(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark])[filtered_face]\n        \n    with ThreadPoolExecutor(max_workers=3) as executor:\n        executor.submit(get_hands, frame)\n        executor.submit(get_pose, frame)\n        executor.submit(get_face, frame)\n\n    return all_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:03:08.363969Z","iopub.execute_input":"2023-12-13T20:03:08.364396Z","iopub.status.idle":"2023-12-13T20:03:08.43972Z","shell.execute_reply.started":"2023-12-13T20:03:08.364362Z","shell.execute_reply":"2023-12-13T20:03:08.438422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Extracting landmarks from a video","metadata":{}},{"cell_type":"code","source":"def get_video_landmarks(video_path, start_frame=1, end_frame=-1):\n    cap = cv2.VideoCapture(video_path)\n    \n    # if the starting is 0\n    if start_frame <= 1:\n        start_frame = 1\n        \n    # if the video is precropped\n    elif start_frame > int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n        start_frame = 1\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n    # if the final frame was not given (-1)    \n    if end_frame < 0: \n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    num_landmarks = HAND_NUM * 2 + POSE_NUM + FACE_NUM\n    all_frame_landmarks = np.zeros((end_frame - start_frame + 1, num_landmarks, 3))\n    frame_index = 1\n    \n    while cap.isOpened() and frame_index <= end_frame:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_index >= start_frame:\n            frame.flags.writeable = False\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_landmarks = get_frame_landmarks(frame)\n            all_frame_landmarks[frame_index - start_frame] = frame_landmarks\n\n        frame_index += 1\n\n    cap.release()\n    hands.reset()\n    pose.reset()\n    face_mesh.reset()\n    return all_frame_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:03:09.853254Z","iopub.execute_input":"2023-12-13T20:03:09.853646Z","iopub.status.idle":"2023-12-13T20:03:09.865239Z","shell.execute_reply.started":"2023-12-13T20:03:09.853617Z","shell.execute_reply":"2023-12-13T20:03:09.863998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Drawing landmarks on a video","metadata":{}},{"cell_type":"code","source":"def draw_landmarks(input_path, output_path, video_landmarks, start_frame=1, end_frame=-1):\n    cap = cv2.VideoCapture(input_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    if start_frame <= 1:\n        start_frame = 1\n    elif start_frame > int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n        start_frame = 1\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if end_frame < 0:\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n    frame_index = 1\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_index >= start_frame and frame_index <= end_frame:\n            frame_landmarks = video_landmarks[frame_index - start_frame]\n            landmarks = [(int(x * width), int(y * height)) for x, y, _ in frame_landmarks]\n            for x, y in landmarks:\n                cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n            out.write(frame)\n        else:\n            # out.write(frame) # Enable if you want the full video\n            pass\n        frame_index += 1\n\n    cap.release()\n    out.release()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:03:12.10114Z","iopub.execute_input":"2023-12-13T20:03:12.101553Z","iopub.status.idle":"2023-12-13T20:03:12.114707Z","shell.execute_reply.started":"2023-12-13T20:03:12.10152Z","shell.execute_reply":"2023-12-13T20:03:12.113504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Visualizing_Landmarks\">\n\n# **Visualizing Landmarks**\n**<h2>(Not important when training)**","metadata":{}},{"cell_type":"markdown","source":"<h3>Test on a frame","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom io import BytesIO\n\nimage_url = 'https://images.unsplash.com/photo-1515294898968-a408405d7674'\nresponse = requests.get(image_url)\nimg = Image.open(BytesIO(response.content))\nimg = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(img[:,:,::-1])\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:03:14.764733Z","iopub.execute_input":"2023-12-13T20:03:14.765181Z","iopub.status.idle":"2023-12-13T20:03:19.004537Z","shell.execute_reply.started":"2023-12-13T20:03:14.765148Z","shell.execute_reply":"2023-12-13T20:03:19.003474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"height, width, _ = img.shape\n\nframe_landmarks = get_frame_landmarks(img[:,:,::-1])\nfor landmark in frame_landmarks:\n    x = int(landmark[0] * width)\n    y = int(landmark[1] * height)\n    cv2.circle(img, (x, y), 10, (0, 255, 0), -1)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(img[:,:,::-1])\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:03:19.006583Z","iopub.execute_input":"2023-12-13T20:03:19.007722Z","iopub.status.idle":"2023-12-13T20:03:22.826447Z","shell.execute_reply.started":"2023-12-13T20:03:19.00766Z","shell.execute_reply":"2023-12-13T20:03:22.82529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Test on video","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/mutemotion-output/WLASL_parsed_data.json', 'r') as json_file:\n    data = json.load(json_file)\n    \ntest = data[40]\nvideo_landmarks = get_video_landmarks(test['video_path'],test['frame_start'],test['frame_end'])\n\noutput_path = '/kaggle/working/landmarks_test.mp4'\ndraw_landmarks(test['video_path'], output_path, video_landmarks, test['frame_start'],test['frame_end'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:45:22.376165Z","iopub.execute_input":"2023-12-12T17:45:22.376565Z","iopub.status.idle":"2023-12-12T17:45:28.094842Z","shell.execute_reply.started":"2023-12-12T17:45:22.37653Z","shell.execute_reply":"2023-12-12T17:45:28.093936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.remove('/kaggle/working/landmarks_test.mp4')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T01:25:47.74273Z","iopub.execute_input":"2023-12-04T01:25:47.743116Z","iopub.status.idle":"2023-12-04T01:25:47.748078Z","shell.execute_reply.started":"2023-12-04T01:25:47.743085Z","shell.execute_reply":"2023-12-04T01:25:47.746875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Data_Encoding\">\n    \n# **Data Encoding**\n**<h2>(Done Once)**","metadata":{}},{"cell_type":"code","source":"npy_dir = '/kaggle/working/landmarks'\nos.makedirs(npy_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:41:26.557079Z","iopub.execute_input":"2023-12-01T21:41:26.557393Z","iopub.status.idle":"2023-12-01T21:41:26.561758Z","shell.execute_reply.started":"2023-12-01T21:41:26.557365Z","shell.execute_reply":"2023-12-01T21:41:26.560867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    for i in tqdm(range(len(data)), ncols=100):\n        npy_path = os.path.join(npy_dir, f'{i}.npy')\n        if os.path.exists(npy_path): continue\n        video_path = data[i]['video_path']\n        start = data[i]['frame_start']\n        end = data[i]['frame_end']\n        \n        try:\n            video_landmarks = get_video_landmarks(video_path, start, end)\n            np.save(npy_path, video_landmarks)\n            \n        except Exception as e:\n            print(f\"\\nError encoding {video_path}\\n{e}\")\n            continue   \n        clear_output(wait=True)\n\nexcept KeyboardInterrupt:\n    print(\"\\nLoading process interrupted by user.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:42:09.695984Z","iopub.execute_input":"2023-12-01T21:42:09.696373Z","iopub.status.idle":"2023-12-01T21:42:09.838056Z","shell.execute_reply.started":"2023-12-01T21:42:09.696343Z","shell.execute_reply":"2023-12-01T21:42:09.836974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"landmarks_dict = {}\n\nfor filename in os.listdir(npy_dir):\n    if filename.endswith('.npy'):\n        key = filename.split('.')[0]\n        landmarks = np.load(os.path.join(npy_dir, filename), allow_pickle=True)\n        landmarks_dict[key] = landmarks\n\nnp.savez_compressed('/kaggle/working/landmarks_V2.npz', **landmarks_dict)\n\nFileLink(r'landmarks_V2.npz')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:42:48.16379Z","iopub.execute_input":"2023-12-01T21:42:48.164484Z","iopub.status.idle":"2023-12-01T21:53:35.00202Z","shell.execute_reply.started":"2023-12-01T21:42:48.164449Z","shell.execute_reply":"2023-12-01T21:53:35.000023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shutil.rmtree(npy_dir)\n# os.remove('/kaggle/working/landmarks_V2.npz')","metadata":{"execution":{"iopub.status.busy":"2023-12-12T15:16:48.8482Z","iopub.execute_input":"2023-12-12T15:16:48.848655Z","iopub.status.idle":"2023-12-12T15:16:48.854646Z","shell.execute_reply.started":"2023-12-12T15:16:48.848618Z","shell.execute_reply":"2023-12-12T15:16:48.853147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Label_Filtering\">\n\n# **Label Filtering**\n**<h2>(Only Once)**","metadata":{}},{"cell_type":"code","source":"import fasttext\nimport fasttext.util\nfasttext.util.download_model('en', if_exists='ignore')\nft = fasttext.load_model('cc.en.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = list(np.unique(Y_train))\nlen(labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:34:28.024306Z","iopub.execute_input":"2023-12-13T20:34:28.024795Z","iopub.status.idle":"2023-12-13T20:34:28.047797Z","shell.execute_reply.started":"2023-12-13T20:34:28.024757Z","shell.execute_reply":"2023-12-13T20:34:28.046693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_labels = {label: ft.get_word_vector(label) for label in labels}\nnp.savez_compressed('labels.npz', **encoded_labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:35:18.882941Z","iopub.execute_input":"2023-12-13T20:35:18.883364Z","iopub.status.idle":"2023-12-13T20:35:19.443149Z","shell.execute_reply.started":"2023-12-13T20:35:18.883334Z","shell.execute_reply":"2023-12-13T20:35:19.442181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ft.get_nearest_neighbors('happy', k=10)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:35:25.028792Z","iopub.execute_input":"2023-12-13T20:35:25.029211Z","iopub.status.idle":"2023-12-13T20:35:42.048749Z","shell.execute_reply.started":"2023-12-13T20:35:25.029178Z","shell.execute_reply":"2023-12-13T20:35:42.047489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine_similarity(v1, v2):\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\nword1 = 'happy'\nword2 = 'pleased'\n\nvector1 = ft.get_word_vector(word1)\nvector2 = ft.get_word_vector(word2)\n\nsimilarity_score = cosine_similarity(vector1, vector2)\nprint(f\"Similarity score between '{word1}' and '{word2}': {similarity_score}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:36:43.211894Z","iopub.execute_input":"2023-12-13T20:36:43.212317Z","iopub.status.idle":"2023-12-13T20:36:43.222108Z","shell.execute_reply.started":"2023-12-13T20:36:43.212285Z","shell.execute_reply":"2023-12-13T20:36:43.220603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_dict = np.load('/kaggle/working/labels.npz', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:36:55.747583Z","iopub.execute_input":"2023-12-13T20:36:55.748003Z","iopub.status.idle":"2023-12-13T20:36:55.782281Z","shell.execute_reply.started":"2023-12-13T20:36:55.74797Z","shell.execute_reply":"2023-12-13T20:36:55.780967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word = 'car'\nword_vector = ft.get_word_vector(word)\n\nsimilarity_scores = []\nfor label, label_vector in labels_dict.items():\n    similarity_scores.append(cosine_similarity(word_vector, label_vector))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:36:59.946818Z","iopub.execute_input":"2023-12-13T20:36:59.947238Z","iopub.status.idle":"2023-12-13T20:37:01.153393Z","shell.execute_reply.started":"2023-12-13T20:36:59.947204Z","shell.execute_reply":"2023-12-13T20:37:01.152476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins = np.linspace(0, 1, 21)\n\nplt.hist(similarity_scores, bins=bins, edgecolor='black')\nplt.title(f'Cosine Similarity Distribution to \"{word}\"')\nplt.xlabel('Cosine Similarity')\nplt.ylabel('Frequency')\nplt.xticks(np.arange(0, 1.1, 0.1))\nplt.grid(axis='y')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:37:03.059063Z","iopub.execute_input":"2023-12-13T20:37:03.059487Z","iopub.status.idle":"2023-12-13T20:37:03.4361Z","shell.execute_reply.started":"2023-12-13T20:37:03.059455Z","shell.execute_reply":"2023-12-13T20:37:03.43474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"taxi_words = [\n    'hello',\n    'goodbye',\n    'thank you',\n    'sorry',\n    'yes',\n    'no',\n    'stop',\n    'go',\n    'left',\n    'right',\n    'ahead',\n    'around',\n    'help',\n    'address',\n    'destination',\n    'time',\n    'money',\n    'cost',\n    'lost',\n    'map',\n    'street',\n    'road'\n]","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:45:04.189054Z","iopub.execute_input":"2023-12-13T20:45:04.189567Z","iopub.status.idle":"2023-12-13T20:45:04.197052Z","shell.execute_reply.started":"2023-12-13T20:45:04.189534Z","shell.execute_reply":"2023-12-13T20:45:04.195855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.4\nsimilar_words_set = set()\ntaxi_vectors = [ft.get_word_vector(word) for word in taxi_words]\n\nfor label, label_vector in labels_dict.items():\n    for taxi_vector in taxi_vectors:\n        similarity_score = cosine_similarity(taxi_vector, label_vector)\n        if similarity_score > threshold:\n            similar_words_set.add(label)\n            \nsimilar_words_list = list(similar_words_set)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:45:08.603409Z","iopub.execute_input":"2023-12-13T20:45:08.604326Z","iopub.status.idle":"2023-12-13T20:45:10.841256Z","shell.execute_reply.started":"2023-12-13T20:45:08.604273Z","shell.execute_reply":"2023-12-13T20:45:10.839978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(similar_words_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:45:10.8438Z","iopub.execute_input":"2023-12-13T20:45:10.844273Z","iopub.status.idle":"2023-12-13T20:45:10.851792Z","shell.execute_reply.started":"2023-12-13T20:45:10.844227Z","shell.execute_reply":"2023-12-13T20:45:10.850531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/filtered_labels.txt', 'w') as file:\n    for label in similar_words_list:\n        file.write(label + '\\n')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:45:10.85337Z","iopub.execute_input":"2023-12-13T20:45:10.853775Z","iopub.status.idle":"2023-12-13T20:45:10.867534Z","shell.execute_reply.started":"2023-12-13T20:45:10.853743Z","shell.execute_reply":"2023-12-13T20:45:10.865963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Data_Loading\">\n    \n# **Data Loading**","metadata":{}},{"cell_type":"code","source":"landmarks_dict = np.load('/kaggle/input/mutemotion-output/landmarks_V3.npz', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:52:36.055192Z","iopub.execute_input":"2023-12-13T20:52:36.055645Z","iopub.status.idle":"2023-12-13T20:52:36.38019Z","shell.execute_reply.started":"2023-12-13T20:52:36.055609Z","shell.execute_reply":"2023-12-13T20:52:36.37874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_dict = np.load('/kaggle/working/labels.npz', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:52:37.657816Z","iopub.execute_input":"2023-12-13T20:52:37.66843Z","iopub.status.idle":"2023-12-13T20:52:37.844253Z","shell.execute_reply.started":"2023-12-13T20:52:37.668382Z","shell.execute_reply":"2023-12-13T20:52:37.839724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/mutemotion-output/WLASL_parsed_data.json', 'r') as json_file:\n    data = json.load(json_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:52:40.874143Z","iopub.execute_input":"2023-12-13T20:52:40.874569Z","iopub.status.idle":"2023-12-13T20:52:41.049038Z","shell.execute_reply.started":"2023-12-13T20:52:40.874536Z","shell.execute_reply":"2023-12-13T20:52:41.046522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Filter Landmarks (Only for V3)","metadata":{}},{"cell_type":"code","source":"filtered_hand = list(range(21))\n\nfiltered_pose = [11, 12, 13, 14, 15, 16]\n\nfiltered_face = [0, 4, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58,\n                 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105,\n                 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154,\n                 155, 157, 158, 159, 160, 161, 162, 163, 172, 173, 176, 178, 181, 185, 191,\n                 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291,\n                 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324,\n                 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380,\n                 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409,\n                 415, 454, 466, 468, 473]\n\nHAND_NUM = len(filtered_hand)\nPOSE_NUM = len(filtered_pose)\nFACE_NUM = len(filtered_face)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:54:02.765232Z","iopub.execute_input":"2023-12-13T20:54:02.765751Z","iopub.status.idle":"2023-12-13T20:54:02.778243Z","shell.execute_reply.started":"2023-12-13T20:54:02.765712Z","shell.execute_reply":"2023-12-13T20:54:02.776986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"landmarks = (\n    [x for x in filtered_hand] +\n    [x + HAND_NUM for x in filtered_hand] +\n    [x + HAND_NUM * 2 for x in filtered_pose] +\n    [x + HAND_NUM * 2 + POSE_NUM for x in filtered_face]\n)\n\nprint(landmarks)\nprint(f'\\nTotal Number: {len(landmarks)}')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:54:03.124746Z","iopub.execute_input":"2023-12-13T20:54:03.125169Z","iopub.status.idle":"2023-12-13T20:54:03.132281Z","shell.execute_reply.started":"2023-12-13T20:54:03.125138Z","shell.execute_reply":"2023-12-13T20:54:03.131157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Filter Data Source","metadata":{}},{"cell_type":"code","source":"# Only from the wlasl-processed dataset\n# keys = [k for k in landmarks_dict.keys() if 'wlasl-processed' in data[int(k)]['video_path']]\n\n# Only from the wlasl2000-resized dataset\n# keys = [k for k in landmarks_dict.keys() if 'wlasl2000-resized' in data[int(k)]['video_path']]","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:54:05.637591Z","iopub.execute_input":"2023-12-13T20:54:05.63806Z","iopub.status.idle":"2023-12-13T20:54:05.642839Z","shell.execute_reply.started":"2023-12-13T20:54:05.638025Z","shell.execute_reply":"2023-12-13T20:54:05.641756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Filter Labels","metadata":{}},{"cell_type":"code","source":"labels = []\nwith open('/kaggle/working/filtered_labels.txt', 'r') as file:\n    labels = file.read().splitlines()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:56:52.127765Z","iopub.execute_input":"2023-12-13T20:56:52.128223Z","iopub.status.idle":"2023-12-13T20:56:52.137281Z","shell.execute_reply.started":"2023-12-13T20:56:52.128187Z","shell.execute_reply":"2023-12-13T20:56:52.136322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:57:19.752312Z","iopub.execute_input":"2023-12-13T20:57:19.752761Z","iopub.status.idle":"2023-12-13T20:57:19.760253Z","shell.execute_reply.started":"2023-12-13T20:57:19.752725Z","shell.execute_reply":"2023-12-13T20:57:19.759154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Loading Function","metadata":{}},{"cell_type":"code","source":"def load_data(split, labels=None, max_labels=None, max_samples=None, landmarks=None, keys=None):\n    \n    if landmarks is None:\n        landmarks = list(range(landmarks_dict['0'].shape[1]))\n    \n    if keys is None:\n        keys = [k for k in landmarks_dict.keys()]\n    \n    if labels is not None:\n        X = [landmarks_dict[k][:, landmarks, :] for k in keys\n             if data[int(k)]['split'] == split and data[int(k)]['gloss'] in labels]\n        Y = [data[int(k)]['gloss'] for k in keys\n             if data[int(k)]['split'] == split and data[int(k)]['gloss'] in labels]\n    \n    elif max_samples is not None:\n        X = [landmarks_dict[k][:, landmarks, :] for k in keys\n             if data[int(k)]['split'] == split][:max_samples]\n        Y = [data[int(k)]['gloss'] for k in keys\n             if data[int(k)]['split'] == split][:max_samples]\n    \n    elif max_labels is not None:\n        label_counts = {}\n        for k in keys:\n            label = data[int(k)]['gloss']\n            label_counts[label] = label_counts.get(label, 0) + 1\n        \n        top_labels = sorted(label_counts, key=label_counts.get, reverse=True)[:max_labels]\n        X = [landmarks_dict[k][:, landmarks, :] for k in keys\n             if data[int(k)]['gloss'] in top_labels and data[int(k)]['split'] == split]\n        Y = [data[int(k)]['gloss'] for k in keys\n             if data[int(k)]['gloss'] in top_labels and data[int(k)]['split'] == split]\n        \n    else:\n        X = [landmarks_dict[k][:, landmarks, :] for k in keys\n             if data[int(k)]['split'] == split]\n        Y = [data[int(k)]['gloss'] for k in keys\n             if data[int(k)]['split'] == split]\n    \n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:57:25.344151Z","iopub.execute_input":"2023-12-13T20:57:25.344568Z","iopub.status.idle":"2023-12-13T20:57:25.361075Z","shell.execute_reply.started":"2023-12-13T20:57:25.344535Z","shell.execute_reply":"2023-12-13T20:57:25.359844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, Y_train = load_data('train', landmarks=landmarks, labels=labels)\nX_val, Y_val = load_data('val', landmarks=landmarks, labels=labels)\nX_test, Y_test = load_data('test', landmarks=landmarks, labels=labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:57:58.829515Z","iopub.execute_input":"2023-12-13T20:57:58.83082Z","iopub.status.idle":"2023-12-13T20:59:06.978428Z","shell.execute_reply.started":"2023-12-13T20:57:58.830779Z","shell.execute_reply":"2023-12-13T20:59:06.977408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train), len(X_val), len(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:59:12.029326Z","iopub.execute_input":"2023-12-13T20:59:12.029764Z","iopub.status.idle":"2023-12-13T20:59:12.037801Z","shell.execute_reply.started":"2023-12-13T20:59:12.029731Z","shell.execute_reply":"2023-12-13T20:59:12.036519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:59:13.747397Z","iopub.execute_input":"2023-12-13T20:59:13.747954Z","iopub.status.idle":"2023-12-13T20:59:13.756279Z","shell.execute_reply.started":"2023-12-13T20:59:13.747917Z","shell.execute_reply":"2023-12-13T20:59:13.755011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(np.unique(Y_train)), len(np.unique(Y_val)), len(np.unique(Y_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:59:17.261486Z","iopub.execute_input":"2023-12-13T20:59:17.261917Z","iopub.status.idle":"2023-12-13T20:59:17.273777Z","shell.execute_reply.started":"2023-12-13T20:59:17.261883Z","shell.execute_reply":"2023-12-13T20:59:17.272806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.all(np.in1d(np.unique(Y_val), np.unique(Y_train))),\\\nnp.all(np.in1d(np.unique(Y_test), np.unique(Y_train)))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:59:18.764396Z","iopub.execute_input":"2023-12-13T20:59:18.76486Z","iopub.status.idle":"2023-12-13T20:59:18.781663Z","shell.execute_reply.started":"2023-12-13T20:59:18.764823Z","shell.execute_reply":"2023-12-13T20:59:18.780462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# free space\ndel landmarks_dict, data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:59:23.921196Z","iopub.execute_input":"2023-12-13T20:59:23.921588Z","iopub.status.idle":"2023-12-13T20:59:24.311302Z","shell.execute_reply.started":"2023-12-13T20:59:23.921557Z","shell.execute_reply":"2023-12-13T20:59:24.310083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Data_Augmentation\">\n    \n# **Data Augmentation**","metadata":{}},{"cell_type":"markdown","source":"<h3> Rotation Augmentations","metadata":{}},{"cell_type":"code","source":"def rotate(data, rotation_matrix):\n    frames, landmarks, _ = data.shape\n    center = np.array([0.5, 0.5, 0])\n    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n    data = data.reshape(-1, 3)\n    data[non_zero] -= center\n    data[non_zero] = np.dot(data[non_zero], rotation_matrix.T)\n    data[non_zero] += center\n    data = data.reshape(frames, landmarks, 3)\n    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n    data[out_of_range] = 0\n    return data\n\ndef rotate_z(data):\n    angle = np.random.choice([np.random.uniform(-30, -10),\n                              np.random.uniform(10, 30)])\n    theta = np.radians(angle)\n    rotation_matrix = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta), np.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    return rotate(data, rotation_matrix)\n\ndef rotate_y(data):\n    angle = np.random.choice([np.random.uniform(-30, -10),\n                              np.random.uniform(10, 30)])\n    theta = np.radians(angle)\n    rotation_matrix = np.array([\n        [np.cos(theta), 0, np.sin(theta)],\n        [0, 1, 0],\n        [-np.sin(theta), 0, np.cos(theta)]\n    ])\n    return rotate(data, rotation_matrix)\n\ndef rotate_x(data):\n    angle = np.random.choice([np.random.uniform(-30, -10),\n                              np.random.uniform(10, 30)])\n    theta = np.radians(angle)\n    rotation_matrix = np.array([\n        [1, 0, 0],\n        [0, np.cos(theta), -np.sin(theta)],\n        [0, np.sin(theta), np.cos(theta)]\n    ])\n    return rotate(data, rotation_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:47:58.881203Z","iopub.execute_input":"2023-12-12T17:47:58.882081Z","iopub.status.idle":"2023-12-12T17:47:58.895479Z","shell.execute_reply.started":"2023-12-12T17:47:58.882048Z","shell.execute_reply":"2023-12-12T17:47:58.89441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Other Augmentations","metadata":{}},{"cell_type":"code","source":"def zoom(data):\n    factor = np.random.uniform(0.8, 1.2)\n    center = np.array([0.5, 0.5])\n    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n    data[non_zero[:, 0], non_zero[:, 1], :2] = (\n        (data[non_zero[:, 0], non_zero[:, 1], :2] - center) * factor + center\n    )\n    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n    data[out_of_range] = 0\n    return data\n\ndef shift(data):\n    x_shift = np.random.uniform(-0.2, 0.2)\n    y_shift = np.random.uniform(-0.2, 0.2)\n    non_zero = np.argwhere(np.any(data[:, :, :2] != 0, axis=2))\n    data[non_zero[:, 0], non_zero[:, 1], 0] += x_shift\n    data[non_zero[:, 0], non_zero[:, 1], 1] += y_shift\n    out_of_range = np.any((data[:, :, :2] < 0) | (data[:, :, :2] > 1), axis=2)\n    data[out_of_range] = 0\n    return data\n\ndef mask(data):\n    frames, landmarks, _ = data.shape\n    num_hands = int(0.3 * 42)\n    num_rest = int(0.6 * (landmarks - 42))\n\n    mask = np.zeros(landmarks, dtype=bool)\n    indices = np.concatenate([\n        np.random.choice(42, num_hands, replace=False),\n        np.random.choice(landmarks - 42, num_rest, replace=False) + 42\n    ])\n    mask[indices] = True\n    data[:, mask] = 0\n    return data\n\ndef hflip(data):\n    data[:, :, 0] = 1 - data[:, :, 0]\n    return data\n\ndef speedup(data):\n    return data[::2]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:47:59.75057Z","iopub.execute_input":"2023-12-12T17:47:59.750966Z","iopub.status.idle":"2023-12-12T17:47:59.764479Z","shell.execute_reply.started":"2023-12-12T17:47:59.750937Z","shell.execute_reply":"2023-12-12T17:47:59.763549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Apply Augmentations","metadata":{}},{"cell_type":"code","source":"def apply_augmentations(data):\n    aug_functions = [rotate_x, rotate_y, rotate_z, zoom, shift, mask, hflip, speedup]\n    np.random.shuffle(aug_functions)\n    counter = 0\n    for fun in aug_functions:\n        if np.random.rand() < 0.5:\n            data = fun(data)\n            counter += 1\n    \n    if counter == 0:\n        data = apply_augmentations(data)\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:48:00.959339Z","iopub.execute_input":"2023-12-12T17:48:00.960085Z","iopub.status.idle":"2023-12-12T17:48:00.965841Z","shell.execute_reply.started":"2023-12-12T17:48:00.960049Z","shell.execute_reply":"2023-12-12T17:48:00.964951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augment(X, Y, num=None):\n    X_aug = X.copy()\n    Y_aug = Y.copy()\n    \n    if num == None:\n        for i in tqdm(range(len(Y)), ncols=100):\n            num_aug = np.random.choice([1, 2, 3])\n            for n in range(num_aug):\n                X_aug.append(apply_augmentations(X[i].copy()))\n                Y_aug.append(Y[i])\n    elif num > 0:\n        for i in tqdm(range(len(Y)), ncols=100):\n            for n in range(num):\n                X_aug.append(apply_augmentations(X[i].copy()))\n                Y_aug.append(Y[i])\n\n    return X_aug, Y_aug","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:48:01.411233Z","iopub.execute_input":"2023-12-12T17:48:01.411542Z","iopub.status.idle":"2023-12-12T17:48:01.418583Z","shell.execute_reply.started":"2023-12-12T17:48:01.411516Z","shell.execute_reply":"2023-12-12T17:48:01.417632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, Y_train = augment(X_train, Y_train, num=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:48:03.015446Z","iopub.execute_input":"2023-12-12T17:48:03.016156Z","iopub.status.idle":"2023-12-12T17:48:17.723421Z","shell.execute_reply.started":"2023-12-12T17:48:03.016115Z","shell.execute_reply":"2023-12-12T17:48:17.72241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train), len(X_train[0]), len(X_train[0][0]), len(X_train[0][0][0])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:48:26.91271Z","iopub.execute_input":"2023-12-12T17:48:26.913522Z","iopub.status.idle":"2023-12-12T17:48:26.920097Z","shell.execute_reply.started":"2023-12-12T17:48:26.913485Z","shell.execute_reply":"2023-12-12T17:48:26.919147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# free space\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:48:28.845905Z","iopub.execute_input":"2023-12-12T17:48:28.846276Z","iopub.status.idle":"2023-12-12T17:48:29.04233Z","shell.execute_reply.started":"2023-12-12T17:48:28.846245Z","shell.execute_reply":"2023-12-12T17:48:29.041243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Data Permutation","metadata":{}},{"cell_type":"code","source":"permutation_train = list(range(len(Y_train)))\nnp.random.shuffle(permutation_train)\nX_train = [X_train[i] for i in permutation_train]\nY_train = [Y_train[i] for i in permutation_train]\n\npermutation_val = list(range(len(Y_val)))\nnp.random.shuffle(permutation_val)\nX_val = [X_val[i] for i in permutation_val]\nY_val = [Y_val[i] for i in permutation_val]\n\npermutation_test = list(range(len(Y_test)))\nnp.random.shuffle(permutation_test)\nX_test = [X_test[i] for i in permutation_test]\nY_test = [Y_test[i] for i in permutation_test]","metadata":{"execution":{"iopub.status.busy":"2023-12-13T20:59:42.11649Z","iopub.execute_input":"2023-12-13T20:59:42.116926Z","iopub.status.idle":"2023-12-13T20:59:42.133682Z","shell.execute_reply.started":"2023-12-13T20:59:42.116893Z","shell.execute_reply":"2023-12-13T20:59:42.132163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Test Augmentation (Not necessary when training)","metadata":{}},{"cell_type":"code","source":"frame_landmarks = np.expand_dims(frame_landmarks, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:48:36.952891Z","iopub.execute_input":"2023-12-12T17:48:36.953285Z","iopub.status.idle":"2023-12-12T17:48:36.95796Z","shell.execute_reply.started":"2023-12-12T17:48:36.953255Z","shell.execute_reply":"2023-12-12T17:48:36.957002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame_landmarks.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:48:37.184391Z","iopub.execute_input":"2023-12-12T17:48:37.184945Z","iopub.status.idle":"2023-12-12T17:48:37.19069Z","shell.execute_reply.started":"2023-12-12T17:48:37.184918Z","shell.execute_reply":"2023-12-12T17:48:37.189745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(12, 6))\nimg = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n\nfor landmark in frame_landmarks[0]:\n    x = int(landmark[0] * img.shape[1])\n    y = int(landmark[1] * img.shape[0])\n    cv2.circle(img, (x, y), 10, (0, 255, 0), -1)\n    \naxs[0].set_title('Original Image')\naxs[0].imshow(img[:, :, ::-1])\naxs[0].axis('off')\n\naugmented_landmarks = apply_augmentations(frame_landmarks.copy())\nimg = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n\nfor landmark in augmented_landmarks[0]:\n    x = int(landmark[0] * img.shape[1])\n    y = int(landmark[1] * img.shape[0])\n    cv2.circle(img, (x, y), 10, (0, 255, 0), -1)\n\naxs[1].set_title('Augmented Image')\naxs[1].imshow(img[:, :, ::-1])\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:48:37.652657Z","iopub.execute_input":"2023-12-12T17:48:37.653504Z","iopub.status.idle":"2023-12-12T17:48:43.276061Z","shell.execute_reply.started":"2023-12-12T17:48:37.653474Z","shell.execute_reply":"2023-12-12T17:48:43.275161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Data_Preprocessing\">\n    \n# **Data Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"<h3>\n    \n**Method 1:** Sequencing","metadata":{}},{"cell_type":"code","source":"def sequences(X, Y, length=30, step=1, pad=0):\n    X_sequences = []\n    Y_sequences = []\n\n    for inputs, label in zip(X, Y):\n        num = inputs.shape[0]\n\n        if num < length:\n            padding = length - num\n            inputs = np.pad(\n            inputs, ((0, padding), (0, 0), (0, 0)),\n            mode='constant', constant_values=pad\n            )\n            num = length\n\n        for start in range(0, num - length + 1, step):\n            end = start + length\n            sequence = inputs[start:end]\n            X_sequences.append(sequence)\n            Y_sequences.append(label)\n\n    X_sequences = np.array(X_sequences)\n    Y_sequences = np.array(Y_sequences)\n    return X_sequences, Y_sequences","metadata":{"execution":{"iopub.status.busy":"2023-12-13T21:00:19.632574Z","iopub.execute_input":"2023-12-13T21:00:19.633007Z","iopub.status.idle":"2023-12-13T21:00:19.643036Z","shell.execute_reply.started":"2023-12-13T21:00:19.632974Z","shell.execute_reply":"2023-12-13T21:00:19.641766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, Y_train = sequences(X_train, Y_train, length=60, step=20, pad=-100)\nX_val, Y_val = sequences(X_val, Y_val, length=60, step=20, pad=-100)\nX_test, Y_test = sequences(X_test, Y_test, length=60, step=20, pad=-100)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T21:00:23.201529Z","iopub.execute_input":"2023-12-13T21:00:23.201956Z","iopub.status.idle":"2023-12-13T21:00:26.246274Z","shell.execute_reply.started":"2023-12-13T21:00:23.201921Z","shell.execute_reply":"2023-12-13T21:00:26.245059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_val.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-13T21:00:26.248916Z","iopub.execute_input":"2023-12-13T21:00:26.249663Z","iopub.status.idle":"2023-12-13T21:00:26.258256Z","shell.execute_reply.started":"2023-12-13T21:00:26.249618Z","shell.execute_reply":"2023-12-13T21:00:26.257086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>\n    \n**Method 2:** Padding","metadata":{}},{"cell_type":"code","source":"def padding(X, Y, length=None, pad=0):\n    if length is None:\n        length = max(len(x) for x in X)\n    \n    X_padded = []\n    for x in X:\n        if len(x) > length:\n            X_padded.append(x[:length]) #truncate\n        else:\n            pad_length = length - len(x)\n            X_padded.append(np.pad(\n                x, ((0, pad_length), (0, 0), (0, 0)),\n                mode='constant', constant_values=pad\n            ))\n            \n    X_padded = np.array(X_padded)\n    Y = np.array(Y)\n    return X_padded, Y","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:16:26.402825Z","iopub.execute_input":"2023-12-12T17:16:26.404028Z","iopub.status.idle":"2023-12-12T17:16:26.412264Z","shell.execute_reply.started":"2023-12-12T17:16:26.403974Z","shell.execute_reply":"2023-12-12T17:16:26.411136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, Y_train = padding(X_train, Y_train, length=200, pad=-100)\nX_val, Y_val = padding(X_val, Y_val, length=200, pad=-100)\nX_test, Y_test = padding(X_test, Y_test, length=200, pad=-100)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:16:40.941213Z","iopub.execute_input":"2023-12-12T17:16:40.941658Z","iopub.status.idle":"2023-12-12T17:17:02.602973Z","shell.execute_reply.started":"2023-12-12T17:16:40.941622Z","shell.execute_reply":"2023-12-12T17:17:02.601712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_val.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:43.767459Z","iopub.execute_input":"2023-12-12T17:18:43.768166Z","iopub.status.idle":"2023-12-12T17:18:43.776269Z","shell.execute_reply.started":"2023-12-12T17:18:43.768122Z","shell.execute_reply":"2023-12-12T17:18:43.775359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>\n    \n**Method 3:** Skipping & Cloning","metadata":{}},{"cell_type":"code","source":"def skipping(landmarks, desired_frames,mode='floor'):\n    frames_num = landmarks.shape[0]\n    if mode == 'floor':\n        skip_factor = math.floor(frames_num / desired_frames)\n    elif mode == 'ceil':\n        skip_factor = math.ceil(frames_num / desired_frames)\n    skipped_landmarks = []\n\n    for i in range(0, frames_num, skip_factor):\n        skipped_landmarks.append(landmarks[i])\n        if len(skipped_landmarks)==desired_frames:\n            break\n\n    return np.array(skipped_landmarks)\n\n# Test\nskip_test = np.zeros((90, 180, 3))\nresult_1 = skipping(skip_test, 60)\nresult_2 = skipping(skip_test, 60,'ceil')\nprint(result_1.shape)\nprint(result_2.shape)   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cloning(landmarks, desired_frames):\n    \n    frames_num = landmarks.shape[0]\n    repeat_factor = math.ceil(desired_frames / frames_num)\n    \n    cloned_list = np.repeat(landmarks, repeat_factor, axis=0)\n    cloned_list = cloned_list[:desired_frames]\n    return cloned_list\n\n# Test\nclone_test = np.random.random((29, 180, 3)) \nresult = cloning(clone_test, 60)\nprint(result.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clone_skip(landmarks_array,desired_frames):\n    reshaped_landmarks = []\n    for landmarks in landmarks_array:\n        frames_number = landmarks.shape[0]\n        \n        if frames_number == desired_frames:\n            reshaped_landmarks.append(landmarks)\n        elif frames_number < desired_frames:\n            reshaped_landmarks.append(cloning(landmarks,desired_frames))\n        elif frames_number > desired_frames:\n            reshaped_landmarks.append(skipping(landmarks,desired_frames))\n    return np.array(reshaped_landmarks)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Label_Encoding\">\n    \n# **Label Encoding**","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\n\nY_train = label_encoder.fit_transform(Y_train)\nY_val = label_encoder.fit_transform(Y_val)\nY_test = label_encoder.fit_transform(Y_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T21:00:30.843414Z","iopub.execute_input":"2023-12-13T21:00:30.843884Z","iopub.status.idle":"2023-12-13T21:00:30.872236Z","shell.execute_reply.started":"2023-12-13T21:00:30.843847Z","shell.execute_reply":"2023-12-13T21:00:30.871084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train","metadata":{"execution":{"iopub.status.busy":"2023-12-13T21:00:31.042542Z","iopub.execute_input":"2023-12-13T21:00:31.042973Z","iopub.status.idle":"2023-12-13T21:00:31.051181Z","shell.execute_reply.started":"2023-12-13T21:00:31.04294Z","shell.execute_reply":"2023-12-13T21:00:31.049485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# free space\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T21:00:34.195377Z","iopub.execute_input":"2023-12-13T21:00:34.195794Z","iopub.status.idle":"2023-12-13T21:00:34.513336Z","shell.execute_reply.started":"2023-12-13T21:00:34.19576Z","shell.execute_reply":"2023-12-13T21:00:34.512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Models\">\n    \n# **Models**","metadata":{}},{"cell_type":"markdown","source":"<h2>Hady's Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Kamel's Model","metadata":{}},{"cell_type":"code","source":"gc.collect()\ntf.keras.backend.clear_session()\nphysical_devices = tf.config.list_physical_devices('GPU')\nfor device in physical_devices:\n    tf.config.experimental.set_memory_growth(device, True)\nphysical_devices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Masking(mask_value=-100, input_shape=(60, 180, 3)),\n    tf.keras.layers.Reshape((60, 180 * 3)),\n\n    tf.keras.layers.Conv1D(64, 1, strides=1, padding='valid', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.DepthwiseConv1D(3, strides=1, padding='valid', depth_multiplier=1, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.Conv1D(64, 1, strides=1, padding='valid', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.DepthwiseConv1D(5, strides=2, padding='valid', depth_multiplier=4, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.MaxPool1D(2, 2),\n\n    tf.keras.layers.Conv1D(256, 1, strides=1, padding='valid', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.DepthwiseConv1D(3, strides=1, padding='valid', depth_multiplier=1, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.Conv1D(256, 1, strides=1, padding='valid', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.DepthwiseConv1D(5, strides=2, padding='valid', depth_multiplier=4, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.GlobalAvgPool1D(),\n    tf.keras.layers.Dropout(rate=0.5),\n\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(rate=0.5),\n\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(rate=0.5),\n\n    tf.keras.layers.Dense(250)\n])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:50:01.333196Z","iopub.execute_input":"2023-12-12T17:50:01.334052Z","iopub.status.idle":"2023-12-12T17:50:04.690472Z","shell.execute_reply.started":"2023-12-12T17:50:01.334014Z","shell.execute_reply":"2023-12-12T17:50:04.689616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_filepath = '/kaggle/working/Kamel_Checkpoints/'\nos.makedirs(checkpoint_filepath, exist_ok=True)\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(checkpoint_filepath, 'model_{epoch:02d}.h5'),\n    save_weights_only=True,\n    save_best_only=True,\n    monitor='val_accuracy',\n    mode='max',\n)\n\nlr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.01,\n    decay_steps=1000,\n    decay_rate=0.9\n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    patience=50,\n    restore_best_weights=True\n)\n\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:50:13.245855Z","iopub.execute_input":"2023-12-12T17:50:13.246847Z","iopub.status.idle":"2023-12-12T17:50:13.285693Z","shell.execute_reply.started":"2023-12-12T17:50:13.246813Z","shell.execute_reply":"2023-12-12T17:50:13.284863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = model.fit(\n    X_train, Y_train,\n    validation_data=(X_val, Y_val),\n    epochs=1000,\n    batch_size=128,\n    callbacks=[model_checkpoint, early_stopping]\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-12T17:50:14.710648Z","iopub.execute_input":"2023-12-12T17:50:14.711011Z","iopub.status.idle":"2023-12-12T17:51:24.624432Z","shell.execute_reply.started":"2023-12-12T17:50:14.710985Z","shell.execute_reply":"2023-12-12T17:51:24.623416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epoch = 90\n# model.load_weights(f'/kaggle/working/Kamel_Checkpoints/model_{epoch}.h5')\ntest_loss, test_accuracy = model.evaluate(X_test, Y_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:52:09.530917Z","iopub.execute_input":"2023-12-12T17:52:09.531333Z","iopub.status.idle":"2023-12-12T17:52:10.055028Z","shell.execute_reply.started":"2023-12-12T17:52:09.5313Z","shell.execute_reply":"2023-12-12T17:52:10.054225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree('/kaggle/working/Kamel_Checkpoints')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T13:35:48.841703Z","iopub.execute_input":"2023-12-13T13:35:48.842175Z","iopub.status.idle":"2023-12-13T13:35:48.866456Z","shell.execute_reply.started":"2023-12-13T13:35:48.84214Z","shell.execute_reply":"2023-12-13T13:35:48.865018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = hist.history['loss']\nval_loss = hist.history['val_loss']\ntrain_acc = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\n\n# Plotting loss\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_loss, 'g', label='Training loss')\nplt.plot(val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plotting accuracy\nplt.subplot(1, 2, 2)\nplt.plot(train_acc, 'g', label='Training accuracy')\nplt.plot(val_acc, 'b', label='Validation accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:52:20.350786Z","iopub.execute_input":"2023-12-12T17:52:20.351784Z","iopub.status.idle":"2023-12-12T17:52:20.883342Z","shell.execute_reply.started":"2023-12-12T17:52:20.351746Z","shell.execute_reply":"2023-12-12T17:52:20.882491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\nimg = plt.imread('model_plot.png')\nplt.figure(figsize=(40, 30))\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:52:27.81434Z","iopub.execute_input":"2023-12-12T17:52:27.81474Z","iopub.status.idle":"2023-12-12T17:52:28.804682Z","shell.execute_reply.started":"2023-12-12T17:52:27.814711Z","shell.execute_reply":"2023-12-12T17:52:28.803695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = '12-12'\nmodel_filepath = '/kaggle/working/Kamel_Models'\nos.makedirs(model_filepath, exist_ok=True)\n\nmodel.save(model_filepath + f'/{name}')\nmodel.save(model_filepath + f'/{name}.h5')\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_filepath + f'/{name}')\nconverter.target_spec.supported_ops = [\n  tf.lite.OpsSet.TFLITE_BUILTINS,\n  tf.lite.OpsSet.SELECT_TF_OPS\n]\ntflite_model = converter.convert()\nwith open(model_filepath + f'/{name}.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:52:42.989845Z","iopub.execute_input":"2023-12-12T17:52:42.990867Z","iopub.status.idle":"2023-12-12T17:52:51.235108Z","shell.execute_reply.started":"2023-12-12T17:52:42.990822Z","shell.execute_reply":"2023-12-12T17:52:51.23421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Nour's Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}