{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\abdok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MediaPipe Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hand = list(range(21))\n",
    "filtered_pose = [0, 2, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "\n",
    "HAND_NUM = len(filtered_hand)\n",
    "POSE_NUM = len(filtered_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp.solutions.hands.Hands()\n",
    "pose = mp.solutions.pose.Pose()\n",
    "\n",
    "def get_frame_landmarks(frame):\n",
    "    \n",
    "    all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM, 3))\n",
    "    \n",
    "    def get_hands(frame):\n",
    "        results_hands = hands.process(frame)\n",
    "        if results_hands.multi_hand_landmarks:\n",
    "            for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n",
    "                if results_hands.multi_handedness[i].classification[0].index == 0: \n",
    "                    all_landmarks[:HAND_NUM, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # right\n",
    "                else:\n",
    "                    all_landmarks[HAND_NUM:HAND_NUM * 2, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # left\n",
    "\n",
    "    def get_pose(frame):\n",
    "        results_pose = pose.process(frame)\n",
    "        if results_pose.pose_landmarks:\n",
    "            all_landmarks[HAND_NUM * 2:HAND_NUM * 2 + POSE_NUM, :] = np.array(\n",
    "                [(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])[filtered_pose]\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        executor.submit(get_hands, frame)\n",
    "        executor.submit(get_pose, frame)\n",
    "\n",
    "    return all_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_mapping_path = \"590_gloss_mapping.json\"\n",
    "index_gloss_mapping_path = \"590_index_gloss_mapping.json\"\n",
    "index_label_mapping_path = \"590_index_label_mapping.json\"\n",
    "\n",
    "gloss_mapping = json.load(open(gloss_mapping_path, \"r\"))\n",
    "index_gloss_mapping = json.load(open(index_gloss_mapping_path, \"r\"))\n",
    "index_label_mapping = json.load(open(index_label_mapping_path, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data):\n",
    "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 120, 55, 3], [1, 590])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = list(map(int, interpreter.get_input_details()[0]['shape']))\n",
    "output_shape = list(map(int, interpreter.get_output_details()[0]['shape']))\n",
    "input_shape, output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Live Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: None\n",
      "Label: and\n",
      "Label: good\n",
      "Label: good\n",
      "Label: welcome\n",
      "Label: welcome\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "sequence = deque(maxlen=input_shape[1])\n",
    "for _ in range(input_shape[1]):\n",
    "    sequence.append(np.zeros((input_shape[2], 3)))\n",
    "\n",
    "step_length = 40\n",
    "TIME_PER_STEP = step_length / 30.0\n",
    "step_time = time.time()\n",
    "frame_time = 0\n",
    "label = ''\n",
    "step = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: continue\n",
    "    \n",
    "    fps = str(int(1 / (time.time() - frame_time)))\n",
    "    frame_time = time.time()\n",
    "    \n",
    "    # frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_rgb.flags.writeable = False\n",
    "    frame_landmarks = get_frame_landmarks(frame_rgb)\n",
    "    \n",
    "    for point in frame_landmarks:\n",
    "        X = int(point[0] * width)\n",
    "        y = int(point[1] * height)\n",
    "        cv2.circle(frame, (X, y), 2, (0, 255, 0), -1)\n",
    "    cv2.putText(frame, fps, (30,60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, label, (30, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)    \n",
    "    \n",
    "    step.append(frame_landmarks)\n",
    "\n",
    "    if time.time() - step_time >= TIME_PER_STEP:\n",
    "        step = np.array(step)\n",
    "        step = np.apply_along_axis(lambda arr: np.interp(np.linspace(0, 1, step_length),\n",
    "                                                         np.linspace(0, 1, arr.shape[0]), arr),\n",
    "                                   axis=0, arr=step)\n",
    "        \n",
    "        sequence.extend(step)\n",
    "        prediction = predict(np.array(sequence))\n",
    "        prediction = prediction.reshape(-1)\n",
    "        # softmax = np.exp(prediction) / np.sum(np.exp(prediction), axis=0)\n",
    "        # print(f'confidence: {softmax.max()}')\n",
    "        # if softmax.max() < 0.5:\n",
    "        #     label = 'None'\n",
    "        # else:\n",
    "        prediction = prediction.argmax()\n",
    "        label = index_label_mapping[str(prediction)]\n",
    "        print(f'Label: {label}')\n",
    "        \n",
    "        step_time = time.time()\n",
    "        step = []\n",
    "        \n",
    "    cv2.imshow(\"Test\", frame)\n",
    "    cv2.setWindowProperty(\"Test\", cv2.WND_PROP_TOPMOST, 1)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test on Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_landmarks(video_path):\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    all_frame_landmarks = []\n",
    "    \n",
    "    while vid.isOpened():\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame.flags.writeable = False\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_landmarks = get_frame_landmarks(frame)\n",
    "        all_frame_landmarks.append(frame_landmarks)\n",
    "\n",
    "    vid.release()\n",
    "    hands.reset()\n",
    "    pose.reset()\n",
    "    return np.array(all_frame_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(X, length, pad=0):\n",
    "    if len(X) > length:\n",
    "        start = (len(X) - length) // 2\n",
    "        end = start + length\n",
    "        X = X[start:end]\n",
    "    else:\n",
    "        pad_before = (length - len(X)) // 2\n",
    "        pad_after = length - len(X) - pad_before\n",
    "        X = np.pad(X, ((pad_before, pad_after), (0, 0), (0, 0)), 'constant', constant_values=pad)   \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording saved.\n",
      "Processing video...\n",
      "Recording saved.\n",
      "Processing video...\n",
      "Recording saved.\n",
      "Processing video...\n",
      "Recording saved.\n",
      "Processing video...\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import messagebox as msg\n",
    "\n",
    "window = tk.Tk()\n",
    "window.title(\"Record Translation\")\n",
    "window.attributes('-topmost', True)\n",
    "window_width = 700\n",
    "window_height = 550\n",
    "screen_width = window.winfo_screenwidth()\n",
    "screen_height = window.winfo_screenheight()\n",
    "x_position = (screen_width - window_width) // 2\n",
    "y_position = (screen_height - window_height) // 2\n",
    "window.geometry(f\"{window_width}x{window_height}+{x_position}+{y_position}\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "canvas = tk.Canvas(window, width=width, height=height)\n",
    "canvas.pack()\n",
    "\n",
    "os.makedirs(\"recordings\", exist_ok=True)\n",
    "is_recording = False\n",
    "is_translating = False\n",
    "output_record = None\n",
    "video_path = \"\"\n",
    "\n",
    "def update():\n",
    "    global is_recording, is_translating, output_record, video_path, cap, canvas\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # frame = cv2.flip(frame, 1)\n",
    "        photo = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # photo = cv2.flip(photo, 1)\n",
    "        photo = ImageTk.PhotoImage(image=Image.fromarray(photo))\n",
    "        canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "        canvas.photo = photo\n",
    "        \n",
    "    if is_recording:\n",
    "        if output_record is not None:\n",
    "            output_record.write(frame)\n",
    "        canvas.create_oval(10, 10, 30, 30, fill=\"red\")\n",
    "    \n",
    "    if is_translating:\n",
    "        btn_record.config(state=\"disabled\")\n",
    "        canvas.delete(\"all\")\n",
    "        canvas.create_text(width // 2, height // 2, text=\"Processing...\", font=(\"Arial\", 30))\n",
    "        canvas.update()\n",
    "        process_video()\n",
    "        btn_record.config(state=\"normal\")\n",
    "        is_translating = False\n",
    "\n",
    "        \n",
    "    window.after(10, update)\n",
    "\n",
    "def toggle_record():\n",
    "    global is_recording\n",
    "    if not is_recording:\n",
    "        start_recording()\n",
    "    else:\n",
    "        stop_recording()\n",
    "\n",
    "btn_record = tk.Button(window, text=\"Start Recording\", width=20, command=toggle_record)\n",
    "btn_record.pack(pady=10)\n",
    "\n",
    "def start_recording():\n",
    "    global output_record, is_recording, is_translating, video_path\n",
    "    is_recording = True\n",
    "    is_translating = False\n",
    "    btn_record.config(text=\"Stop Recording\")\n",
    "    video_path = f\"recordings/record_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.mp4\"\n",
    "    output_record = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))\n",
    "\n",
    "def stop_recording():\n",
    "    global output_record, is_recording, is_translating\n",
    "    is_recording = False\n",
    "    is_translating = True\n",
    "    btn_record.config(text=\"Start Recording\")\n",
    "    if output_record is not None:\n",
    "        output_record.release()\n",
    "        output_record = None\n",
    "        print(\"Recording saved.\")\n",
    "    \n",
    "def process_video():\n",
    "    global video_path, cap\n",
    "    print(\"Processing video...\")\n",
    "    landmarks = get_video_landmarks(video_path)\n",
    "    landmarks = padding(landmarks, input_shape[1])\n",
    "    prediction = predict(landmarks)\n",
    "    prediction = prediction.reshape(-1)\n",
    "    prediction = prediction.argmax()\n",
    "    label = index_label_mapping[str(prediction)]\n",
    "    msg.showinfo(\"Processing Completed\", \"Translation: \" + label)\n",
    "    \n",
    "update()\n",
    "window.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Testing Videos/8507947567336565-AMAZING.mp4\" controls  width=\"640\"  height=\"480\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = \"Testing Videos/8507947567336565-AMAZING.mp4\"\n",
    "\n",
    "from IPython.display import Video\n",
    "Video(video_path, width=640, height=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: amazing\n"
     ]
    }
   ],
   "source": [
    "landmarks = get_video_landmarks(video_path)\n",
    "landmarks = padding(landmarks, input_shape[1])\n",
    "prediction = predict(landmarks)\n",
    "prediction = prediction.reshape(-1)\n",
    "prediction = prediction.argmax()\n",
    "label = index_label_mapping[str(prediction)]\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
